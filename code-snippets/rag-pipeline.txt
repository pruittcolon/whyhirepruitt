"""
RAG (Retrieval-Augmented Generation) Pipeline
services/rag-service/src/rag_pipeline.py

Context injection features:
- Semantic document retrieval
- Context window optimization
- Memory management
- Query augmentation
"""

import numpy as np
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class RetrievedContext:
    text: str
    score: float
    source: str
    metadata: Dict

@dataclass
class AugmentedPrompt:
    prompt: str
    contexts: List[RetrievedContext]
    token_count: int

class RAGPipeline:
    """Retrieval-Augmented Generation for grounded LLM responses."""
    
    def __init__(self, embedding_service, vector_store):
        self.embedder = embedding_service
        self.store = vector_store
        self.max_context_tokens = 4096
        
    async def retrieve(
        self, 
        query: str, 
        top_k: int = 5,
        min_score: float = 0.7
    ) -> List[RetrievedContext]:
        """Retrieve relevant documents for query."""
        
        # Encode query
        query_embedding = self.embedder.encode_query(query)
        
        # Search vector store
        results = self.store.search(query_embedding, k=top_k * 2)
        
        # Filter by score and deduplicate
        contexts = []
        seen_texts = set()
        
        for doc_id, score, metadata in results:
            if score >= min_score and metadata['text'] not in seen_texts:
                contexts.append(RetrievedContext(
                    text=metadata['text'],
                    score=score,
                    source=metadata.get('source', 'unknown'),
                    metadata=metadata
                ))
                seen_texts.add(metadata['text'])
                
                if len(contexts) >= top_k:
                    break
                    
        logger.info(f"Retrieved {len(contexts)} contexts for query")
        return contexts
    
    async def augment_prompt(
        self, 
        query: str,
        system_prompt: str,
        contexts: List[RetrievedContext]
    ) -> AugmentedPrompt:
        """Build prompt with retrieved context."""
        
        # Rank and select contexts that fit in window
        context_text = self._build_context_section(contexts)
        
        # Construct augmented prompt
        prompt = f"""{system_prompt}

## Relevant Context:
{context_text}

## User Query:
{query}

## Response:"""
        
        return AugmentedPrompt(
            prompt=prompt,
            contexts=contexts,
            token_count=len(prompt.split()) * 1.3  # Rough token estimate
        )
    
    def _build_context_section(self, contexts: List[RetrievedContext]) -> str:
        """Format contexts for prompt injection."""
        sections = []
        for i, ctx in enumerate(contexts, 1):
            sections.append(f"[{i}] (Source: {ctx.source}, Relevance: {ctx.score:.2f})\n{ctx.text}")
        return "\n\n".join(sections)
    
    async def query(self, user_query: str, system_prompt: str = "") -> Tuple[str, List[RetrievedContext]]:
        """Full RAG pipeline: retrieve + augment."""
        contexts = await self.retrieve(user_query)
        augmented = await self.augment_prompt(user_query, system_prompt, contexts)
        return augmented.prompt, contexts
