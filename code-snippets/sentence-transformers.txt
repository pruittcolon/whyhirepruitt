"""
Sentence Transformers Embedding Service
services/rag-service/src/embeddings.py

Semantic embedding features:
- MiniLM text embeddings
- Batch processing
- Cosine similarity search
- Query/document encoding
"""

from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class EmbeddingService:
    """Neural text embeddings for semantic similarity."""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.dimension = 384  # MiniLM embedding size
        
    def load(self):
        """Load sentence transformer model."""
        logger.info(f"Loading embedding model: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"Model loaded. Embedding dimension: {self.dimension}")
        
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Encode texts to embeddings."""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            normalize_embeddings=True
        )
        return embeddings
    
    def encode_query(self, query: str) -> np.ndarray:
        """Encode single query with query-specific prefix."""
        return self.model.encode(
            f"query: {query}",
            normalize_embeddings=True
        )
    
    def encode_document(self, document: str) -> np.ndarray:
        """Encode document text."""
        return self.model.encode(
            f"passage: {document}",
            normalize_embeddings=True
        )
    
    def similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """Compute cosine similarity between embeddings."""
        return float(np.dot(emb1, emb2))
    
    def find_similar(
        self, 
        query_emb: np.ndarray, 
        corpus_embs: np.ndarray, 
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """Find top-k most similar embeddings."""
        similarities = np.dot(corpus_embs, query_emb)
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        return [(int(idx), float(similarities[idx])) for idx in top_indices]
    
    def chunk_and_embed(
        self, 
        text: str, 
        chunk_size: int = 512, 
        overlap: int = 50
    ) -> Tuple[List[str], np.ndarray]:
        """Split text into chunks and embed each."""
        chunks = []
        words = text.split()
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        embeddings = self.encode(chunks)
        return chunks, embeddings
