"""
PostgreSQL Task Queue Implementation
services/queue-service/src/postgres_queue.py

Durable queue features:
- Crash-safe job persistence
- Transaction isolation
- Priority ordering
- Retry handling
"""

import asyncpg
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)

@dataclass
class Job:
    id: int
    task_type: str
    payload: Dict
    priority: int
    status: str
    created_at: datetime
    attempts: int

class PostgresQueue:
    """PostgreSQL-backed durable job queue."""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
        
    async def connect(self):
        """Initialize connection pool."""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=2,
            max_size=10
        )
        await self._create_tables()
        logger.info("PostgreSQL queue connected")
        
    async def _create_tables(self):
        """Create queue tables if not exist."""
        async with self.pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS job_queue (
                    id SERIAL PRIMARY KEY,
                    task_type VARCHAR(100) NOT NULL,
                    payload JSONB NOT NULL,
                    priority INT DEFAULT 0,
                    status VARCHAR(20) DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW(),
                    attempts INT DEFAULT 0,
                    error TEXT
                );
                CREATE INDEX IF NOT EXISTS idx_queue_status ON job_queue(status, priority DESC);
            """)
    
    async def enqueue(self, task_type: str, payload: Dict, priority: int = 0) -> int:
        """Add job to queue."""
        async with self.pool.acquire() as conn:
            job_id = await conn.fetchval("""
                INSERT INTO job_queue (task_type, payload, priority)
                VALUES ($1, $2, $3)
                RETURNING id
            """, task_type, json.dumps(payload), priority)
            
        logger.info(f"Enqueued job {job_id}: {task_type}")
        return job_id
    
    async def dequeue(self) -> Optional[Job]:
        """Get next pending job atomically."""
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow("""
                UPDATE job_queue
                SET status = 'processing', updated_at = NOW()
                WHERE id = (
                    SELECT id FROM job_queue
                    WHERE status = 'pending'
                    ORDER BY priority DESC, created_at ASC
                    FOR UPDATE SKIP LOCKED
                    LIMIT 1
                )
                RETURNING *
            """)
            
        if row:
            return Job(
                id=row['id'],
                task_type=row['task_type'],
                payload=json.loads(row['payload']),
                priority=row['priority'],
                status=row['status'],
                created_at=row['created_at'],
                attempts=row['attempts']
            )
        return None
    
    async def complete(self, job_id: int):
        """Mark job as completed."""
        async with self.pool.acquire() as conn:
            await conn.execute("""
                UPDATE job_queue SET status = 'completed', updated_at = NOW()
                WHERE id = $1
            """, job_id)
